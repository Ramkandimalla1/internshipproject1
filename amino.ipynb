{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'table-transformer' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/microsoft/table-transformer\n",
    "!cd table-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd table-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\venvs\\transformers_env\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\venvs\\transformers_env\\lib\\site-packages (from pdf2image) (11.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\venvs\\transformers_env\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\venvs\\transformers_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: timm in c:\\venvs\\transformers_env\\lib\\site-packages (1.0.11)\n",
      "Requirement already satisfied: torch in c:\\venvs\\transformers_env\\lib\\site-packages (from timm) (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\venvs\\transformers_env\\lib\\site-packages (from timm) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in c:\\venvs\\transformers_env\\lib\\site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\venvs\\transformers_env\\lib\\site-packages (from timm) (0.26.2)\n",
      "Requirement already satisfied: safetensors in c:\\venvs\\transformers_env\\lib\\site-packages (from timm) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (24.2)\n",
      "Requirement already satisfied: requests in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface_hub->timm) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\venvs\\transformers_env\\lib\\site-packages (from torch->timm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch->timm) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\venvs\\transformers_env\\lib\\site-packages (from torch->timm) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch->timm) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\venvs\\transformers_env\\lib\\site-packages (from torchvision->timm) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from torchvision->timm) (11.0.0)\n",
      "Requirement already satisfied: colorama in c:\\venvs\\transformers_env\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from jinja2->torch->timm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->huggingface_hub->timm) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->huggingface_hub->timm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->huggingface_hub->timm) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  pip <command> [options]\n",
      "\n",
      "Commands:\n",
      "  install                     Install packages.\n",
      "  download                    Download packages.\n",
      "  uninstall                   Uninstall packages.\n",
      "  freeze                      Output installed packages in requirements format.\n",
      "  inspect                     Inspect the python environment.\n",
      "  list                        List installed packages.\n",
      "  show                        Show information about installed packages.\n",
      "  check                       Verify installed packages have compatible dependencies.\n",
      "  config                      Manage local and global configuration.\n",
      "  search                      Search PyPI for packages.\n",
      "  cache                       Inspect and manage pip's wheel cache.\n",
      "  index                       Inspect information available from package indexes.\n",
      "  wheel                       Build wheels from your requirements.\n",
      "  hash                        Compute hashes of package archives.\n",
      "  completion                  A helper command used for command completion.\n",
      "  debug                       Show information useful for debugging.\n",
      "  help                        Show help for commands.\n",
      "\n",
      "General Options:\n",
      "  -h, --help                  Show help.\n",
      "  --debug                     Let unhandled exceptions propagate outside the\n",
      "                              main subroutine, instead of logging them to\n",
      "                              stderr.\n",
      "  --isolated                  Run pip in an isolated mode, ignoring\n",
      "                              environment variables and user configuration.\n",
      "  --require-virtualenv        Allow pip to only run in a virtual environment;\n",
      "                              exit with an error otherwise.\n",
      "  --python <python>           Run pip with the specified Python interpreter.\n",
      "  -v, --verbose               Give more output. Option is additive, and can be\n",
      "                              used up to 3 times.\n",
      "  -V, --version               Show version and exit.\n",
      "  -q, --quiet                 Give less output. Option is additive, and can be\n",
      "                              used up to 3 times (corresponding to WARNING,\n",
      "                              ERROR, and CRITICAL logging levels).\n",
      "  --log <path>                Path to a verbose appending log.\n",
      "  --no-input                  Disable prompting for input.\n",
      "  --keyring-provider <keyring_provider>\n",
      "                              Enable the credential lookup via the keyring\n",
      "                              library if user input is allowed. Specify which\n",
      "                              mechanism to use [disabled, import, subprocess].\n",
      "                              (default: disabled)\n",
      "  --proxy <proxy>             Specify a proxy in the form\n",
      "                              scheme://[user:passwd@]proxy.server:port.\n",
      "  --retries <retries>         Maximum number of retries each connection should\n",
      "                              attempt (default 5 times).\n",
      "  --timeout <sec>             Set the socket timeout (default 15 seconds).\n",
      "  --exists-action <action>    Default action when a path already exists:\n",
      "                              (s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\n",
      "  --trusted-host <hostname>   Mark this host or host:port pair as trusted,\n",
      "                              even though it does not have valid or any HTTPS.\n",
      "  --cert <path>               Path to PEM-encoded CA certificate bundle. If\n",
      "                              provided, overrides the default. See 'SSL\n",
      "                              Certificate Verification' in pip documentation\n",
      "                              for more information.\n",
      "  --client-cert <path>        Path to SSL client certificate, a single file\n",
      "                              containing the private key and the certificate\n",
      "                              in PEM format.\n",
      "  --cache-dir <dir>           Store the cache data in <dir>.\n",
      "  --no-cache-dir              Disable the cache.\n",
      "  --disable-pip-version-check\n",
      "                              Don't periodically check PyPI to determine\n",
      "                              whether a new version of pip is available for\n",
      "                              download. Implied with --no-index.\n",
      "  --no-color                  Suppress colored output.\n",
      "  --no-python-version-warning\n",
      "                              Silence deprecation warnings for upcoming\n",
      "                              unsupported Pythons.\n",
      "  --use-feature <feature>     Enable new functionality, that may be backward\n",
      "                              incompatible.\n",
      "  --use-deprecated <feature>  Enable deprecated functionality, that will be\n",
      "                              removed in the future.\n"
     ]
    }
   ],
   "source": [
    "!pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\venvs\\transformers_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\venvs\\transformers_env\\lib\\site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\venvs\\transformers_env\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\venvs\\transformers_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\venvs\\transformers_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\venvs\\transformers_env\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: pdf2image in c:\\venvs\\transformers_env\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pandas in c:\\venvs\\transformers_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow in c:\\venvs\\transformers_env\\lib\\site-packages (from pdf2image) (11.0.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torch pdf2image pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-detection were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, AutoTokenizer\n",
    "from pdf2image import convert_from_path\n",
    "import pandas as pd\n",
    "\n",
    "# Load the pre-trained TATR model and tokenizer\n",
    "model_name = \"microsoft/table-transformer-detection\"\n",
    "model = TableTransformerForObjectDetection.from_pretrained(model_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PDF pages to images\n",
    "def convert_pdf_to_images(pdf_path, dpi=300):\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "    return images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Function to run table detection using the TATR model\n",
    "def run_table_detection(image, model):\n",
    "    inputs = tokenizer(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Process model outputs\n",
    "    table_boxes = outputs.pred_boxes\n",
    "    return table_boxes\n",
    "\n",
    "# Function to extract tables from images\n",
    "def extract_tables_from_images(images, model):\n",
    "    table_data = []\n",
    "    for image in images:\n",
    "        boxes = run_table_detection(image, model)\n",
    "        table_data.append(boxes)\n",
    "    return table_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save the extracted tables to CSV\n",
    "def save_tables_to_csv(tables, output_path):\n",
    "    for i, table in enumerate(tables):\n",
    "        table_df = pd.DataFrame(table)  # Convert table data to DataFrame\n",
    "        csv_filename = f\"{output_path}/table_{i+1}.csv\"\n",
    "        table_df.to_csv(csv_filename, index=False)\n",
    "        print(f\"Table {i+1} saved to {csv_filename}\")\n",
    "\n",
    "# Function to save the extracted tables to Excel\n",
    "def save_tables_to_excel(tables, output_path):\n",
    "    writer = pd.ExcelWriter(f\"{output_path}/tables_output.xlsx\")\n",
    "    for i, table in enumerate(tables):\n",
    "        table_df = pd.DataFrame(table)\n",
    "        table_df.to_excel(writer, sheet_name=f'Table_{i+1}', index=False)\n",
    "    writer.save()\n",
    "    print(\"Tables saved to Excel file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: poppler-utils in c:\\venvs\\transformers_env\\lib\\site-packages (0.1.0)\n",
      "Requirement already satisfied: Click>=7.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from poppler-utils) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\venvs\\transformers_env\\lib\\site-packages (from Click>=7.0->poppler-utils) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install poppler-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\venvs\\transformers_env\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\venvs\\transformers_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from pdf2image import convert_from_path\n",
    "\n",
    "# # Specify the path to poppler if needed (only if it’s not in PATH)\n",
    "#poppler_path = r'C:\\poppler\\poppler-24.08.0\\Library\\bin'  # Replace with the actual path on Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-detection were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table extraction completed! Enlarged tables are saved in D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput_table\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, DetrImageProcessor\n",
    "from pdf2image import convert_from_path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Function to enlarge the bounding box\n",
    "def enlarge_box(box, scale_factor=1.4):\n",
    "    # Center of the box\n",
    "    x_center = (box[0] + box[2]) / 2\n",
    "    y_center = (box[1] + box[3]) / 2\n",
    "    width = (box[2] - box[0]) * scale_factor\n",
    "    height = (box[3] - box[1]) * scale_factor\n",
    "\n",
    "    # New coordinates\n",
    "    x_min = x_center - width / 2\n",
    "    y_min = y_center - height / 2\n",
    "    x_max = x_center + width / 2\n",
    "    y_max = y_center + height / 2\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "# Function to run table detection using the TATR model\n",
    "def run_table_detection(image, model, processor):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted boxes\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "\n",
    "    return results\n",
    "\n",
    "# Function to extract tables from a PDF\n",
    "def extract_tables_from_pdf(pdf_path, output_dir, model, processor, scale_factor=1.2):\n",
    "    # Convert PDF pages to images\n",
    "    images = convert_from_path(pdf_path, poppler_path=r\"C:\\poppler\\poppler-24.08.0\\Library\\bin\")\n",
    "\n",
    "    for page_number, image in enumerate(images):\n",
    "        # Run table detection on each page\n",
    "        results = run_table_detection(image, model, processor)\n",
    "\n",
    "        # Plot detected tables\n",
    "        fig, ax = plt.subplots(1, figsize=(16, 10))\n",
    "        ax.imshow(image)\n",
    "\n",
    "        for box in results['boxes'].detach().numpy():  # Detach tensor before converting to numpy\n",
    "            # Enlarge the bounding box by the scale factor\n",
    "            enlarged_box = enlarge_box(box, scale_factor)\n",
    "\n",
    "            # Add rectangle to the plot for the detected table\n",
    "            ax.add_patch(plt.Rectangle((enlarged_box[0], enlarged_box[1]), enlarged_box[2] - enlarged_box[0], enlarged_box[3] - enlarged_box[1],\n",
    "                                       fill=False, color=\"red\", linewidth=3))\n",
    "\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(output_dir, f\"page_{page_number+1}_tables.png\"))\n",
    "        plt.close()\n",
    "\n",
    "# Load the Table Transformer model and image processor\n",
    "model_name = \"microsoft/table-transformer-detection\"\n",
    "model = TableTransformerForObjectDetection.from_pretrained(model_name)\n",
    "processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Replace pdf_path with your actual file path on Windows\n",
    "# pdf_path = r\"D:\\InternshipProject\\projects\\content\\sample_data\\1.pdf\"  # Use raw string (r\"\") to avoid escape issues\n",
    "output_dir = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\enlargeoutput_table\"  # Also update the output directory if needed\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Extract tables and save them\n",
    "extract_tables_from_pdf(pdf_path, output_dir, model, processor, scale_factor=1.2)\n",
    "\n",
    "print(f\"Table extraction completed! Enlarged tables are saved in {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\venvs\\transformers_env\\lib\\site-packages (from pytesseract) (24.2)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from pytesseract) (11.0.0)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: pytesseract\n",
      "Successfully installed pytesseract-0.3.13\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract tables image from full images-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "enlarge tables images-enlargeoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/table-transformer-detection were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput created.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import TableTransformerForObjectDetection, DetrImageProcessor\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "# Function to enlarge the bounding box\n",
    "def enlarge_box(box, scale_factor=1.2):\n",
    "    # Center of the box\n",
    "    x_center = (box[0] + box[2]) / 2\n",
    "    y_center = (box[1] + box[3]) / 2\n",
    "    width = (box[2] - box[0]) * scale_factor\n",
    "    height = (box[3] - box[1]) * scale_factor\n",
    "\n",
    "    # New coordinates\n",
    "    x_min = x_center - width / 2\n",
    "    y_min = y_center - height / 2\n",
    "    x_max = x_center + width / 2\n",
    "    y_max = y_center + height / 2\n",
    "\n",
    "    return [x_min, y_min, x_max, y_max]\n",
    "\n",
    "# Function to run table detection using the TATR model\n",
    "def run_table_detection(image, model, processor):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get predicted boxes\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    return results\n",
    "\n",
    "# Function to crop detected table regions, enlarge, and save as images\n",
    "def crop_and_save_tables(image, results, output_dir, page_number, scale_factor=1.2):\n",
    "    tables_data = []\n",
    "    for i, box in enumerate(results['boxes'].detach().numpy()):\n",
    "        # Enlarge the detected table box\n",
    "        enlarged_box = enlarge_box(box, scale_factor)\n",
    "\n",
    "        # Crop the enlarged table region\n",
    "        cropped_image = image.crop((enlarged_box[0], enlarged_box[1], enlarged_box[2], enlarged_box[3]))\n",
    "        cropped_image_path = os.path.join(output_dir, f\"page_{page_number+1}_table_{i+1}.png\")\n",
    "        cropped_image.save(cropped_image_path)\n",
    "\n",
    "        # Convert cropped table image to text using OCR (Tesseract)\n",
    "        table_text = pytesseract.image_to_string(cropped_image, config=\"--psm 6\")\n",
    "        tables_data.append(table_text)\n",
    "    return tables_data\n",
    "\n",
    "# Function to extract tables from a PDF and save them as CSV or Excel\n",
    "def extract_tables_to_excel(pdf_path, output_dir, model, processor, scale_factor=1.2):\n",
    "    images = convert_from_path(pdf_path, poppler_path=r\"C:\\poppler\\poppler-24.08.0\\Library\\bin\")\n",
    "    all_tables = []\n",
    "\n",
    "    for page_number, image in enumerate(images):\n",
    "        results = run_table_detection(image, model, processor)\n",
    "        tables_data = crop_and_save_tables(image, results, output_dir, page_number, scale_factor)\n",
    "\n",
    "        for table_text in tables_data:\n",
    "            table_rows = [row.split() for row in table_text.splitlines() if row.strip()]\n",
    "            all_tables.extend(table_rows)\n",
    "\n",
    "    df = pd.DataFrame(all_tables)\n",
    "    output_file_path = os.path.join(output_dir, \"extracted_tables.xlsx\")\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    return output_file_path\n",
    "\n",
    "# Load the Table Transformer model and image processor\n",
    "model_name = \"microsoft/table-transformer-detection\"\n",
    "model = TableTransformerForObjectDetection.from_pretrained(model_name)\n",
    "processor = DetrImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Replace pdf_path with your actual file path on Windows\n",
    "# pdf_path = r\"D:\\InternshipProject\\projects\\content\\sample_data\\1.pdf\"  # Use raw string (r\"\") to avoid escape issues\n",
    "output_dir = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\enlargeoutput\"  # Also update the output directory if needed\n",
    "\n",
    "# Create output directory if it does not exist\n",
    "if not os.makedirs(output_dir, exist_ok=True):\n",
    "    print(f\"Directory {output_dir} created.\")\n",
    "\n",
    "# # Install tesseract-ocr if not installed\n",
    "# !apt-get install -y tesseract-ocr\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "\n",
    "# # Extract tables and save them as Excel\n",
    "output_file = extract_tables_to_excel(pdf_path, output_dir, model, processor, scale_factor=1.2)\n",
    "\n",
    "# # Provide the generated Excel file to the user\n",
    "# output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image to csv-enlargecsv_folder\n",
    "NEXT STAGES: \n",
    "after careful observation we came to know that csv tables extarct is not correct so to increase effeciency i am using extract table api to get csv from images\n",
    "extraction of tables from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'ExtractTable-py'...\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ExtractTable/ExtractTable-py.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ExtractTable\n",
      "  Downloading ExtractTable-2.4.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: requests>=2.21 in c:\\venvs\\transformers_env\\lib\\site-packages (from ExtractTable) (2.32.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\venvs\\transformers_env\\lib\\site-packages (from ExtractTable) (2.2.3)\n",
      "Collecting PyPDF2>=1.26 (from ExtractTable)\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas>=0.24->ExtractTable) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas>=0.24->ExtractTable) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas>=0.24->ExtractTable) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\venvs\\transformers_env\\lib\\site-packages (from pandas>=0.24->ExtractTable) (2024.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests>=2.21->ExtractTable) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests>=2.21->ExtractTable) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests>=2.21->ExtractTable) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\venvs\\transformers_env\\lib\\site-packages (from requests>=2.21->ExtractTable) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in c:\\venvs\\transformers_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.24->ExtractTable) (1.16.0)\n",
      "Downloading ExtractTable-2.4.0-py3-none-any.whl (19 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2, ExtractTable\n",
      "Successfully installed ExtractTable-2.4.0 PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ExtractTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use ur own api :https://extracttable.com/signup/trial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput\\page_1_table_1.png\n",
      "Table saved to: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutputcs\\table_page_1_table_1.png_part_1.csv\n",
      "Processing image: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput\\page_3_table_1.png\n",
      "Table saved to: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutputcs\\table_page_3_table_1.png_part_1.csv\n",
      "Processing image: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput\\page_4_table_1.png\n",
      "Table saved to: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutputcs\\table_page_4_table_1.png_part_1.csv\n",
      "Processing image: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput\\page_4_table_2.png\n",
      "Table saved to: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutputcs\\table_page_4_table_2.png_part_1.csv\n",
      "Processing image: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutput\\page_5_table_1.png\n",
      "Table saved to: D:\\InternshipProject\\projects\\content\\poorna\\enlargeoutputcs\\table_page_5_table_1.png_part_1.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ExtractTable import ExtractTable\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize ExtractTable API client with your API key\n",
    "api_key = \"k97NYypymJdUCrwc30GeHgjyiumiirC6aiVa9WK7\"  # Replace this with your actual API key\n",
    "et_sess = ExtractTable(api_key=api_key)\n",
    "\n",
    "# Function to extract tables from images and save them as CSV\n",
    "def extract_tables_from_images(image_folder_path, output_folder):\n",
    "    # Get list of all image file paths in the folder\n",
    "    image_paths = [os.path.join(image_folder_path, file) for file in os.listdir(image_folder_path) if file.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        try:\n",
    "            print(f\"Processing image: {image_path}\")\n",
    "\n",
    "            # Extract tables from the image using ExtractTable API\n",
    "            table_data = et_sess.process_file(image_path)\n",
    "\n",
    "            if table_data:\n",
    "                for i, table in enumerate(table_data):\n",
    "                    # Convert the table data to a DataFrame\n",
    "                    df = pd.DataFrame(table)\n",
    "\n",
    "                    # Define output file path\n",
    "                    output_csv_path = os.path.join(output_folder, f\"table_{os.path.basename(image_path)}_part_{i+1}.csv\")\n",
    "\n",
    "                    # Save the DataFrame as a CSV file\n",
    "                    df.to_csv(output_csv_path, index=False)\n",
    "                    print(f\"Table saved to: {output_csv_path}\")\n",
    "            else:\n",
    "                print(f\"No table detected in {image_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\n",
    "# Paths\n",
    "image_folder_path = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\enlargeoutput\"  # Update this with the actual folder path containing images\n",
    "output_folder = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\enlargeoutputcs\"  # Update this with the folder where you want to save CSVs\n",
    "\n",
    "# Extract tables from images in the folder and save as CSV\n",
    "extract_tables_from_images(image_folder_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removing tables if more than 80 % conatins text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table_page_1_table_1.png_part_1.csv\n",
      "Non-digit percentage for table_page_1_table_1.png_part_1.csv: 100.00%\n",
      "Deleted table_page_1_table_1.png_part_1.csv: More than 15% of cells are non-digit.\n",
      "Processing table_page_3_table_1.png_part_1.csv\n",
      "Non-digit percentage for table_page_3_table_1.png_part_1.csv: 21.90%\n",
      "Moved table_page_3_table_1.png_part_1.csv to D:\\InternshipProject\\projects\\content\\poorna\\tables\n",
      "Processing table_page_4_table_1.png_part_1.csv\n",
      "Non-digit percentage for table_page_4_table_1.png_part_1.csv: 37.00%\n",
      "Moved table_page_4_table_1.png_part_1.csv to D:\\InternshipProject\\projects\\content\\poorna\\tables\n",
      "Processing table_page_4_table_2.png_part_1.csv\n",
      "Non-digit percentage for table_page_4_table_2.png_part_1.csv: 60.00%\n",
      "Moved table_page_4_table_2.png_part_1.csv to D:\\InternshipProject\\projects\\content\\poorna\\tables\n",
      "Processing table_page_5_table_1.png_part_1.csv\n",
      "Non-digit percentage for table_page_5_table_1.png_part_1.csv: 36.36%\n",
      "Moved table_page_5_table_1.png_part_1.csv to D:\\InternshipProject\\projects\\content\\poorna\\tables\n",
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\3605188218.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\3605188218.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\3605188218.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\3605188218.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\3605188218.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "input_folder = r'C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\enlargeoutputcs'  # Folder containing CSV files\n",
    "output_folder = r'C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tables'  # Folder to move valid CSV files\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Function to calculate the percentage of non-digit cells\n",
    "def percentage_non_digit_cells(df):\n",
    "    total_cells = df.size\n",
    "    non_digit_cells = df.applymap(lambda x: not str(x).replace('.', '', 1).isdigit()).sum().sum()\n",
    "    return (non_digit_cells / total_cells) * 100\n",
    "\n",
    "# Iterate through all CSV files in the folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(input_folder, file_name)\n",
    "        print(f\"Processing {file_name}\")\n",
    "\n",
    "        try:\n",
    "            # Load the CSV to check its contents\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Calculate the percentage of non-digit cells\n",
    "            non_digit_percentage = percentage_non_digit_cells(df)\n",
    "            print(f\"Non-digit percentage for {file_name}: {non_digit_percentage:.2f}%\")\n",
    "\n",
    "            # Check if the percentage of non-digit cells is less than or equal to 15%\n",
    "            if non_digit_percentage <= 80:\n",
    "                # Move the file to the output folder if it's valid\n",
    "                shutil.copy(file_path, os.path.join(output_folder, file_name))\n",
    "                print(f\"Moved {file_name} to {output_folder}\")\n",
    "            else:\n",
    "                # Delete the file if more than 15% of cells are non-digit\n",
    "\n",
    "                print(f\"Deleted {file_name}: More than 15% of cells are non-digit.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding sample data header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed files: ['table_page_3_table_1.png_part_1.csv', 'table_page_4_table_1.png_part_1.csv', 'table_page_4_table_2.png_part_1.csv', 'table_page_5_table_1.png_part_1.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tables\"\n",
    "output_folder_path = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tableheaders\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder_path):\n",
    "    os.makedirs(output_folder_path)\n",
    "\n",
    "# Loop through all CSV files in the folder\n",
    "for file_name in os.listdir(csv_folder_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        file_path = os.path.join(csv_folder_path, file_name)\n",
    "\n",
    "        # Read the CSV file, skipping the first row (header row in original)\n",
    "        df = pd.read_csv(file_path, header=None, skiprows=1)\n",
    "\n",
    "        # Promote the first row to be the header\n",
    "        df.columns = df.iloc[0]\n",
    "        df = df[1:]  # Remove the row that became the header\n",
    "\n",
    "        # Add \"Sample\" as the first header\n",
    "        df.columns = ['Sample'] + df.columns.tolist()[1:]\n",
    "\n",
    "        # Save the updated file to the new folder\n",
    "        output_file_path = os.path.join(output_folder_path, file_name)\n",
    "        df.to_csv(output_file_path, index=False)\n",
    "\n",
    "# List the processed files\n",
    "processed_files = os.listdir(output_folder_path)\n",
    "print(\"Processed files:\", processed_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting final data: getting all correct csv tables according to pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been processed and saved to the new directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5948\\2548881133.py:27: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the original CSV files and the new directory for modified files\n",
    "input_directory = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tableheaders\"\n",
    "output_directory =r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\finaltable\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Iterate through each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Load the CSV file\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        csv_data = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if 'Sample' column has the 'Casein Red Lentil' entry and proceed with modifications\n",
    "        if 'Casein Red Lentil' in csv_data['Sample'].values:\n",
    "            # Replace \"Casein Red Lentil\" with \"Casein\"\n",
    "            csv_data['Sample'] = csv_data['Sample'].replace('Casein Red Lentil', 'Casein')\n",
    "            # Find the index of the modified row\n",
    "            index = csv_data[csv_data['Sample'] == 'Casein'].index[0]\n",
    "            # Insert \"Red Lentil\" in the next row, clear other cells if the row exists\n",
    "            if index + 1 < len(csv_data):\n",
    "                csv_data.loc[index + 1, 'Sample'] = 'Red Lentil'\n",
    "                csv_data.loc[index + 1, csv_data.columns != 'Sample'] = ''\n",
    "            else:\n",
    "                # Add a new row if there is no next row\n",
    "                new_row = pd.Series({'Sample': 'Red Lentil'})\n",
    "                csv_data = csv_data.append(new_row, ignore_index=True).fillna('')\n",
    "\n",
    "        # Save the modified CSV file to the new directory\n",
    "        output_file_path = os.path.join(output_directory, filename)\n",
    "        csv_data.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"All files have been processed and saved to the new directory.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "next step :It reads the CSV, applies suffixes to subsequent rows based on specific single-cell entries, removes those single-cell rows, and saves the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: table_page_3_table_1.png_part_1.csv\n",
      "Processed and saved: table_page_4_table_1.png_part_1.csv\n",
      "Processed and saved: table_page_4_table_2.png_part_1.csv\n",
      "Processed and saved: table_page_5_table_1.png_part_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def apply_suffix_and_clean_data(csv_data):\n",
    "    # Variable to hold the current suffix to apply\n",
    "    current_suffix = None\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in csv_data.iterrows():\n",
    "        # Check if the row has only one non-empty cell and it's in the 'Sample' column\n",
    "        if row.count() == 1 and pd.notna(row['Sample']):\n",
    "            current_suffix = row['Sample']  # Update the current suffix\n",
    "        elif current_suffix and pd.notna(row['Sample']):\n",
    "            # Apply the current suffix to the 'Sample' column if a suffix is present\n",
    "            csv_data.at[index, 'Sample'] += f\" {current_suffix}\"\n",
    "\n",
    "    # Remove rows where only the 'Sample' column is non-empty\n",
    "    cleaned_csv_data = csv_data.dropna(thresh=2)  # 'thresh=2' means at least two non-NA values\n",
    "\n",
    "    return cleaned_csv_data\n",
    "\n",
    "def process_csv_files(input_directory, output_directory):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Iterate through each file in the input directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            csv_data = pd.read_csv(file_path)\n",
    "\n",
    "            # Apply suffixes and clean the data\n",
    "            modified_data = apply_suffix_and_clean_data(csv_data)\n",
    "\n",
    "            # Save the modified CSV file to the new directory\n",
    "            output_file_path = os.path.join(output_directory, filename)\n",
    "            modified_data.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {filename}\")\n",
    "\n",
    "# Specify the directories\n",
    "input_directory = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\finaltable\"\n",
    "output_directory = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tablesuffixed\"\n",
    "\n",
    "# Process all CSV files in the directory\n",
    "process_csv_files(input_directory, output_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\venvs\\transformers_env\\lib\\site-packages (from pdfplumber) (11.0.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\venvs\\transformers_env\\lib\\site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.0)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six==20231228->pdfplumber)\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ----------------------------- ---------- 4.2/5.6 MB 25.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.6/5.6 MB 22.9 MB/s eta 0:00:00\n",
      "Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 56.0 MB/s eta 0:00:00\n",
      "Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 87.7 MB/s eta 0:00:00\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: pypdfium2, pycparser, cffi, cryptography, pdfminer.six, pdfplumber\n",
      "Successfully installed cffi-1.17.1 cryptography-43.0.3 pdfminer.six-20231228 pdfplumber-0.11.4 pycparser-2.22 pypdfium2-4.30.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://doi.org/10.1016/j.foodchem.2017.07.129\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "# Function to extract DOI from a PDF and create a link\n",
    "def extract_doi_link(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regex to find DOI\n",
    "                match = re.search(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    # Create a link using the DOI\n",
    "                    return f\"https://doi.org/{match.group(0)}\"\n",
    "    return \"DOI not found\"\n",
    "\n",
    "# Example usage\n",
    "# pdf_path = r\"D:\\InternshipProject\\projects\\content\\sample_data\\1.pdf\"\n",
    "doi_link = extract_doi_link(pdf_path)\n",
    "print(doi_link)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Excel.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "# Define amino acid mapping\n",
    "amino_acid_mapping = {\n",
    "    # Single letter and full name mappings to three-letter codes\n",
    "    'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU', 'F': 'PHE',\n",
    "    'G': 'GLY', 'H': 'HIS', 'I': 'ILE', 'K': 'LYS', 'L': 'LEU',\n",
    "    'M': 'MET', 'N': 'ASN', 'P': 'PRO', 'Q': 'GLN', 'R': 'ARG',\n",
    "    'S': 'SER', 'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR',\n",
    "    'Alanine': 'ALA', 'Cysteine': 'CYS', 'Aspartic Acid': 'ASP',\n",
    "    'Glutamic Acid': 'GLU', 'Phenylalanine': 'PHE', 'Glycine': 'GLY',\n",
    "    'Histidine': 'HIS', 'Isoleucine': 'ILE', 'Lysine': 'LYS', 'Leucine': 'LEU',\n",
    "    'Methionine': 'MET', 'Asparagine': 'ASN', 'Proline': 'PRO',\n",
    "    'Glutamine': 'GLN', 'Arginine': 'ARG', 'Serine': 'SER',\n",
    "    'Threonine': 'THR', 'Valine': 'VAL', 'Tryptophan': 'TRP', 'Tyrosine': 'TYR'\n",
    "}\n",
    "\n",
    "required_headers = ['SAMPLE', 'ASP', 'THR', 'SER', 'GLU', 'PRO', 'GLY',\n",
    "                    'ALA', 'CYS', 'VAL', 'MET', 'ILE', 'LEU', 'TYR', 'PHE', 'HIS',\n",
    "                    'LYS', 'ARG', 'TRP']\n",
    "\n",
    "def extract_doi_link(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regex to find DOI\n",
    "                match = re.search(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    # Create a link using the DOI\n",
    "                    return f\"https://doi.org/{match.group(0)}\"\n",
    "    return \"DOI not found\"\n",
    "\n",
    "def standardize_column_names(cols):\n",
    "    mapping = {key.upper(): value for key, value in amino_acid_mapping.items()}\n",
    "    return {col: mapping.get(col, col) for col in cols}\n",
    "\n",
    "def process_csv_files(directory, doi):\n",
    "    csv_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    data_frames = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        mapped_names = standardize_column_names(df.columns)\n",
    "        df.rename(columns=mapped_names, inplace=True)\n",
    "        df = df[[col for col in df.columns if col in required_headers]]\n",
    "        data_frames.append((df, sum(df.columns.isin(required_headers))))\n",
    "\n",
    "    data_frames.sort(key=lambda x: x[1], reverse=True)\n",
    "    primary_df = data_frames[0][0]\n",
    "\n",
    "    for df, _ in data_frames[1:]:\n",
    "        for column in df.columns:\n",
    "            if column in primary_df.columns and primary_df[column].isnull().all():\n",
    "                primary_df[column] = df[column]\n",
    "\n",
    "    if doi:\n",
    "        primary_df['DOI'] = doi  # Add the DOI as a new column in the DataFrame\n",
    "\n",
    "    return primary_df.loc[:, required_headers + ['DOI'] if doi else required_headers]\n",
    "\n",
    "# Example usage\n",
    "# pdf_path = r\"D:\\InternshipProject\\projects\\content\\sample_data\\1.pdf\"\n",
    "directory_path = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tablesuffixed\"\n",
    "doi = extract_doi_link(pdf_path)\n",
    "\n",
    "try:\n",
    "    final_data = process_csv_files(directory_path, doi)\n",
    "    final_data.to_excel('C:/Users/kandi/Desktop/projects/projects/content/poorna/tablesuffixed/fdoi.xlsx', index=False)\n",
    "    print(\"Data successfully saved to Excel.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to Excel.\n",
      "Data .\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "# Define amino acid mapping\n",
    "amino_acid_mapping = {\n",
    "    # Single letter and full name mappings to three-letter codes\n",
    "    'A': 'ALA', 'C': 'CYS', 'D': 'ASP', 'E': 'GLU', 'F': 'PHE',\n",
    "    'G': 'GLY', 'H': 'HIS', 'I': 'ILE', 'K': 'LYS', 'L': 'LEU',\n",
    "    'M': 'MET', 'N': 'ASN', 'P': 'PRO', 'Q': 'GLN', 'R': 'ARG',\n",
    "    'S': 'SER', 'T': 'THR', 'V': 'VAL', 'W': 'TRP', 'Y': 'TYR',\n",
    "    'Alanine': 'ALA', 'Cysteine': 'CYS', 'Aspartic Acid': 'ASP',\n",
    "    'Glutamic Acid': 'GLU', 'Phenylalanine': 'PHE', 'Glycine': 'GLY',\n",
    "    'Histidine': 'HIS', 'Isoleucine': 'ILE', 'Lysine': 'LYS', 'Leucine': 'LEU',\n",
    "    'Methionine': 'MET', 'Asparagine': 'ASN', 'Proline': 'PRO',\n",
    "    'Glutamine': 'GLN', 'Arginine': 'ARG', 'Serine': 'SER',\n",
    "    'Threonine': 'THR', 'Valine': 'VAL', 'Tryptophan': 'TRP', 'Tyrosine': 'TYR'\n",
    "}\n",
    "\n",
    "required_headers = ['SAMPLE', 'ASP', 'THR', 'SER', 'GLU', 'PRO', 'GLY',\n",
    "                    'ALA', 'CYS', 'VAL', 'MET', 'ILE', 'LEU', 'TYR', 'PHE', 'HIS',\n",
    "                    'LYS', 'ARG', 'TRP']\n",
    "\n",
    "def extract_doi_link(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                # Regex to find DOI\n",
    "                match = re.search(r'\\b10\\.\\d{4,9}/[-._;()/:A-Z0-9]+\\b', text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    # Create a link using the DOI\n",
    "                    return f\"https://doi.org/{match.group(0)}\"\n",
    "    return \"DOI not found\"\n",
    "\n",
    "def extract_title(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        first_page = pdf.pages[0]\n",
    "        text = first_page.extract_text()\n",
    "        if text:\n",
    "            # Assume the title is the first line of the text\n",
    "            lines = text.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if any(name in line for name in ['Matthew G. Nosworthy', 'Adam Franczyk']):  # Names of first authors\n",
    "                    # Return the line just above the author names as the title\n",
    "                    title_start = max(i - 3, 0)  # Ensure not to go out of index\n",
    "                    title_end = i\n",
    "                    return ' '.join(lines[title_start:title_end]).strip()\n",
    "    return \"Title not found\"\n",
    "\n",
    "def standardize_column_names(cols):\n",
    "    mapping = {key.upper(): value for key, value in amino_acid_mapping.items()}\n",
    "    return {col: mapping.get(col, col) for col in cols}\n",
    "\n",
    "def process_csv_files(directory, doi, title):\n",
    "    csv_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "    data_frames = []\n",
    "\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        df.columns = [col.upper() for col in df.columns]\n",
    "        mapped_names = standardize_column_names(df.columns)\n",
    "        df.rename(columns=mapped_names, inplace=True)\n",
    "        df = df[[col for col in df.columns if col in required_headers]]\n",
    "        df['TITLE'] = title  # Add the title as a new column in the DataFrame\n",
    "        data_frames.append((df, sum(df.columns.isin(required_headers))))\n",
    "\n",
    "    data_frames.sort(key=lambda x: x[1], reverse=True)\n",
    "    primary_df = data_frames[0][0]\n",
    "\n",
    "    for df, _ in data_frames[1:]:\n",
    "        for column in df.columns:\n",
    "            if column in primary_df.columns and primary_df[column].isnull().all():\n",
    "                primary_df[column] = df[column]\n",
    "\n",
    "    if doi:\n",
    "        primary_df['DOI'] = doi\n",
    "\n",
    "    return primary_df.loc[:, required_headers + ['DOI', 'TITLE'] if doi else required_headers + ['TITLE']]\n",
    "\n",
    "# Example usage\n",
    "# pdf_path = r\"D:\\InternshipProject\\projects\\content\\sample_data\\1.pdf\"\n",
    "directory_path = r\"C:\\Users\\kandi\\Desktop\\projects\\projects\\content\\poorna\\tablesuffixed\"\n",
    "doi = extract_doi_link(pdf_path)\n",
    "title = extract_title(pdf_path)\n",
    "\n",
    "try:\n",
    "    final_data = process_csv_files(directory_path, doi, title)\n",
    "    final_data.to_excel('C:/Users/kandi/Desktop/projects/projects/content/poorna/tablesuffixed/fd.xlsx', index=False)\n",
    "    print(\"Data successfully saved to Excel.\")\n",
    "    print(\"Data .\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
